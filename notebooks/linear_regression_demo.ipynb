{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Linear Regression\n",
    "\n",
    "Linear regression is a foundational supervised learning method for modeling the relationship between an input variable and a real-valued output. In its simplest (univariate) form, we assume the relationship between an input x and target y is approximately linear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and notation\n",
    "- We observe a training set of m examples:  \n",
    "  $\\mathcal{D} = \\{(x^{(i)},\\, y^{(i)})\\}_{i=1}^m$ where $x^{(i)} \\in \\mathbb{R}$ and $y^{(i)} \\in \\mathbb{R}$.\n",
    "- For the univariate case (one feature), $x^{(i)}$ is a scalar. For the multivariate case with n features, $x^{(i)} \\in \\mathbb{R}^n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model (hypothesis)\n",
    "For the univariate case, our model (also called the hypothesis) is a straight line:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    " f_{w,b}(x) &= w\\,x + b\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "- $w$ is the slope (weight), and $b$ is the intercept (bias).\n",
    "- Multivariate generalization: with $x \\in \\mathbb{R}^n$ and $w \\in \\mathbb{R}^n$,\n",
    "\n",
    "$$\n",
    " f_{w,b}(x) = w^\\top x + b.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost function (Mean Squared Error)\n",
    "We measure how well a particular $(w,b)$ fits the training data using the Mean Squared Error (MSE) cost function:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "J(w,b) &= \\frac{1}{2m} \\sum_{i=1}^{m} \\Big(f_{w,b}(x^{(i)}) - y^{(i)}\\Big)^2 \\\\\n",
    "       &= \\frac{1}{2m} \\sum_{i=1}^{m} \\Big( w\\,x^{(i)} + b - y^{(i)} \\Big)^2.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "- The factor $\\tfrac{1}{2}$ is included so that derivatives are slightly cleaner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivatives (gradients)\n",
    "To minimize $J(w,b)$, we compute its partial derivatives with respect to $w$ and $b$.\n",
    "\n",
    "Univariate derivatives:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial J}{\\partial w} &= \\frac{1}{m} \\sum_{i=1}^{m} \\Big( w\\,x^{(i)} + b - y^{(i)} \\Big)\\, x^{(i)} \\\\\n",
    "\\frac{\\partial J}{\\partial b} &= \\frac{1}{m} \\sum_{i=1}^{m} \\Big( w\\,x^{(i)} + b - y^{(i)} \\Big).\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "For multivariate case with $X \\in \\mathbb{R}^{m\\times n}$ (rows $(x^{(i)})^\\top$) and $\\hat{y} = Xw + b\\,\\mathbf{1}$, the derivatives generalize to matrix form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "Gradient descent is an iterative optimization algorithm that updates parameters in the direction of the negative gradient to reduce the cost.\n",
    "\n",
    "Initialize $w$ and $b$ (e.g., to 0), choose a learning rate $\\alpha>0$, and repeat for a chosen number of iterations $T$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    " w &\\leftarrow w - \\alpha\\, \\frac{\\partial J}{\\partial w} \\\\\n",
    " b &\\leftarrow b - \\alpha\\, \\frac{\\partial J}{\\partial b}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Pseudocode (univariate):\n",
    "\n",
    "```\n",
    "input: data {(x^(i), y^(i))}_{i=1}^m, learning rate α, iterations T\n",
    "initialize: w ← 0, b ← 0\n",
    "for t in {1,…,T}:\n",
    "    compute gradients:\n",
    "        dw = (1/m) * Σ_i (w*x^(i) + b − y^(i)) * x^(i)\n",
    "        db = (1/m) * Σ_i (w*x^(i) + b − y^(i))\n",
    "    update:\n",
    "        w ← w − α * dw\n",
    "        b ← b − α * db\n",
    "return w, b\n",
    "```\n",
    "\n",
    "### Notes on practice\n",
    "- Learning rate (α) controls the step size. Too large can diverge; too small can be slow.\n",
    "- Feature scaling (for multivariate x) often improves convergence.\n",
    "- Convergence can be monitored by plotting $J(w,b)$ over iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Implementation: Linear Regression Demo\n",
    "\n",
    "Now let's implement this from scratch using Python! We'll generate synthetic data, implement gradient descent, and visualize the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Import Libraries\n",
    "\n",
    "First, we need to import the necessary libraries for our implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "# Enable inline plotting\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Generate Synthetic Data\n",
    "\n",
    "We'll create synthetic data from a known linear relationship with some added noise. This allows us to verify that our algorithm recovers the true parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_data(m, true_w, true_b, noise_std):\n",
    "    \"\"\"Generate y = true_w * x + true_b + noise.\"\"\"\n",
    "    x = []\n",
    "    y = []\n",
    "    for i in range(m):\n",
    "        xi = -5 + (10 / (m - 1)) * i  # linspace equivalent\n",
    "        noise = random.gauss(0, noise_std)\n",
    "        yi = true_w * xi + true_b + noise\n",
    "        x.append(xi)\n",
    "        y.append(yi)\n",
    "    return x, y, true_w, true_b\n",
    "\n",
    "# Generate synthetic data\n",
    "x, y, true_w, true_b = generate_synthetic_data(m=120, true_w=3.2, true_b=-0.7, noise_std=1.2)\n",
    "print(f\"Generated {len(x)} data points\")\n",
    "print(f\"True parameters: w={true_w:.3f}, b={true_b:.3f}\")\n",
    "\n",
    "# Visualize the data\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(x, y, color=\"royalblue\", alpha=0.7, label=\"data points\")\n",
    "plt.title(\"Synthetic Data\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Implement the Cost Function\n",
    "\n",
    "The cost function $J(w,b)$ measures how well our parameters fit the data using Mean Squared Error (MSE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(x, y, w, b):\n",
    "    \"\"\"Compute MSE cost J(w,b) = (1/(2m)) * sum((w*x_i + b - y_i)^2)\"\"\"\n",
    "    m = len(x)\n",
    "    total_error_sq = 0.0\n",
    "    for i in range(m):\n",
    "        prediction = w * x[i] + b\n",
    "        error = prediction - y[i]\n",
    "        total_error_sq += error ** 2\n",
    "    return total_error_sq / (2 * m)\n",
    "\n",
    "# Test with initial parameters\n",
    "initial_cost = compute_cost(x, y, w=0.0, b=0.0)\n",
    "print(f\"Initial cost (w=0, b=0): {initial_cost:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Implement Gradient Computation\n",
    "\n",
    "We need to compute the partial derivatives (gradients) of the cost function with respect to $w$ and $b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradients(x, y, w, b):\n",
    "    \"\"\"Compute dw and db: dw = (1/m) * sum((w*x_i + b - y_i) * x_i), db = (1/m) * sum(w*x_i + b - y_i)\"\"\"\n",
    "    m = len(x)\n",
    "    dw = 0.0\n",
    "    db = 0.0\n",
    "    for i in range(m):\n",
    "        prediction = w * x[i] + b\n",
    "        error = prediction - y[i]\n",
    "        dw += error * x[i]\n",
    "        db += error\n",
    "    dw /= m\n",
    "    db /= m\n",
    "    return dw, db\n",
    "\n",
    "# Test gradient computation\n",
    "dw, db = compute_gradients(x, y, w=0.0, b=0.0)\n",
    "print(f\"Initial gradients: dw={dw:.4f}, db={db:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Implement Gradient Descent\n",
    "\n",
    "Now we'll implement the gradient descent algorithm to iteratively update our parameters and minimize the cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(x, y, alpha, iterations):\n",
    "    \"\"\"Run gradient descent to find w and b.\"\"\"\n",
    "    w = 0.0\n",
    "    b = 0.0\n",
    "    cost_history = []\n",
    "    for it in range(iterations):\n",
    "        cost = compute_cost(x, y, w, b)\n",
    "        cost_history.append(cost)\n",
    "        dw, db = compute_gradients(x, y, w, b)\n",
    "        w -= alpha * dw\n",
    "        b -= alpha * db\n",
    "        # Early stopping if cost change is tiny\n",
    "        if it > 0 and abs(cost_history[-2] - cost) < 1e-12:\n",
    "            break\n",
    "    return w, b, cost_history\n",
    "\n",
    "# Run gradient descent\n",
    "alpha = 0.01  # Learning rate\n",
    "iterations = 1500\n",
    "w, b, cost_history = gradient_descent(x, y, alpha, iterations)\n",
    "\n",
    "print(f\"Learned parameters: w={w:.3f}, b={b:.3f}\")\n",
    "print(f\"Final cost: {cost_history[-1]:.4f} (after {len(cost_history)} iterations)\")\n",
    "print(f\"\\nComparison:\")\n",
    "print(f\"  True:    w={true_w:.3f}, b={true_b:.3f}\")\n",
    "print(f\"  Learned: w={w:.3f}, b={b:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Visualize the Results\n",
    "\n",
    "Let's visualize both the fitted line and how the cost decreased over iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, w, b):\n",
    "    \"\"\"Predict y for given x: y = w * x + b\"\"\"\n",
    "    predictions = []\n",
    "    for xi in x:\n",
    "        predictions.append(w * xi + b)\n",
    "    return predictions\n",
    "\n",
    "# Create figure with two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Data with fitted line\n",
    "ax1.scatter(x, y, color=\"royalblue\", alpha=0.7, label=\"data\")\n",
    "x_line = []\n",
    "x_min = min(x)\n",
    "x_max = max(x)\n",
    "for i in range(200):\n",
    "    xi = x_min + (x_max - x_min) / 199 * i\n",
    "    x_line.append(xi)\n",
    "y_line = predict(x_line, w, b)\n",
    "ax1.plot(x_line, y_line, color=\"darkorange\", linewidth=2, label=\"fitted line\")\n",
    "ax1.set_title(\"Linear Regression Fit\")\n",
    "ax1.set_xlabel(\"x\")\n",
    "ax1.set_ylabel(\"y\")\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Cost history\n",
    "ax2.plot(cost_history, color=\"purple\", linewidth=2)\n",
    "ax2.set_title(\"Cost vs Iteration\")\n",
    "ax2.set_xlabel(\"Iteration\")\n",
    "ax2.set_ylabel(\"Cost J(w,b)\")\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Make Predictions\n",
    "\n",
    "Now we can use our learned model to make predictions on new data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on example points\n",
    "example_x = [0.0, 2.0, -3.0]\n",
    "preds = predict(example_x, w, b)\n",
    "\n",
    "print(\"Predictions using learned model:\")\n",
    "for xv, yv in zip(example_x, preds):\n",
    "    print(f\"  f({xv: .1f}) = {yv: .3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises for Students\n",
    "\n",
    "Try modifying the parameters and see how they affect the results:\n",
    "\n",
    "1. **Learning rate experiments**: Try different values of `alpha` (e.g., 0.001, 0.1, 0.5). What happens?\n",
    "2. **Noise level**: Change `noise_std` in the data generation. How does it affect the fit?\n",
    "3. **Data size**: Try generating different numbers of data points (e.g., 30, 500). How does this affect convergence?\n",
    "4. **True parameters**: Generate data with different true values of `true_w` and `true_b`. Does the algorithm still converge?\n",
    "5. **Early stopping**: Observe when the cost stops changing significantly. Could we stop earlier?\n",
    "\n",
    "### Challenge\n",
    "Extend this implementation to handle multiple features (multivariate linear regression)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Save Plots to Files\n",
    "\n",
    "If you want to save the plots as image files, you can use the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_plots(x, y, w, b, cost_history, out_dir):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    # Plot data + fitted line\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.scatter(x, y, color=\"royalblue\", alpha=0.7, label=\"data\")\n",
    "    x_line = []\n",
    "    x_min = min(x)\n",
    "    x_max = max(x)\n",
    "    for i in range(200):\n",
    "        xi = x_min + (x_max - x_min) / 199 * i\n",
    "        x_line.append(xi)\n",
    "    y_line = predict(x_line, w, b)\n",
    "    plt.plot(x_line, y_line, color=\"darkorange\", label=\"fitted line\")\n",
    "    plt.title(\"Linear Regression Fit\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    fit_path = os.path.join(out_dir, \"data_scatter_fit.png\")\n",
    "    plt.savefig(fit_path, dpi=120)\n",
    "    plt.close()\n",
    "\n",
    "    # Plot cost history\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.plot(cost_history, color=\"purple\")\n",
    "    plt.title(\"Cost vs Iteration\")\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Cost J(w,b)\")\n",
    "    plt.tight_layout()\n",
    "    cost_path = os.path.join(out_dir, \"cost_history.png\")\n",
    "    plt.savefig(cost_path, dpi=120)\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"Saved plots:\\n  {fit_path}\\n  {cost_path}\")\n",
    "\n",
    "# Uncomment the line below to save plots\n",
    "# save_plots(x, y, w, b, cost_history, \"../outputs\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
